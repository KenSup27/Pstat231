---
title: "Pstat231HW6"
author: "Wentao Yu"
date: "2022-11-16"
output: 
      html_document:
        toc: true
        toc_float: true
        code_folding: show
---
```{r include=FALSE}
library(xgboost)
library(vip)
library(ranger)
library(rpart.plot)
library(corrplot)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
library(janitor)
library(glmnet)
library(ISLR)
library(ISLR2)
tidymodels_prefer()
```

## Question 1
```{r}
## table reading
df0 <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-5/data/Pokemon.csv')
df <- clean_names(df0)
df1 <- dplyr::filter(df, type_1 %in% c('Bug','Fire','Grass','Normal','Water','Psychic')) %>% 
  mutate(type_1 = factor(type_1),
         legendary = factor(legendary),
         generation = factor(generation)) ## select the required outcomes and factor type_1, legendary, and generation. 
```
```{r}
set.seed(721) ## set seed to stabilize the outcome
pokemon.split <- initial_split(df1, prop = 0.8, strata = type_1)
pokemon.training <- training(pokemon.split)
pokemon.testing <- testing(pokemon.split)
pokemon.fold <- vfold_cv(pokemon.training, v=5, strata = type_1) ## return the distribution of strata variable to each fold.
```
```{r}
pokemon.recipe <- recipe(type_1 ~ legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data = pokemon.training) %>% 
  step_dummy(c(legendary,generation)) %>% 
  step_scale(all_predictors()) %>% 
  step_center(all_predictors())
```

## Question 2
```{r}
cor.pokemon <- pokemon.training %>% 
  select(-c(total, name, type_1, type_2, generation, legendary)) %>%  # eliminate chr variables
  correlate() # create correlation data frame
rplot(cor.pokemon) # create correlation plot
```
Explanation: all the variables are positive correlated with each others. sp_def and defense are higher correlated with each other. sp_atk and attack are higher correlated with each other. 

## Question 3
```{r}
tree.pokemon <- decision_tree() %>% 
  set_engine('rpart') %>% 
  set_mode('classification')
pokemon_wkflow <- workflow() %>% 
  add_model(tree.pokemon %>% set_args(cost_complexity = tune())) %>% 
  add_recipe(pokemon.recipe)
param_grid <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)
tune_tree <- tune_grid(
  pokemon_wkflow,
  resamples = pokemon.fold,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)
autoplot(tune_tree)
```
Explanation: Higher cost-complexity parameter leads to lower roc_auc(lower performance)

## Question 4
```{r}
collect_metrics(tune_tree) 
best_model <- select_best(tune_tree, metric = 'roc_auc')
pokemon_tree_best <- finalize_workflow(pokemon_wkflow, best_model)
pokemon_tree_best_fit <- fit(pokemon_tree_best, data = pokemon.training)
pruned_tree <- augment(pokemon_tree_best_fit, pokemon.training, type = 'prob') %>% 
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
pruned_tree
```
Explanation: 

## Question 5
```{r}
pokemon_tree_best_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot(roundint = F) 
```
mtry should less than the predictors in the data set. 

## Question 6
```{r}
pokemon_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine('ranger', importance = "impurity") %>% 
  set_mode('classification')
pokemon_forest_wkflow <- workflow() %>% 
  add_model(pokemon_forest_model) %>% 
  add_recipe(pokemon.recipe)
param1_grid <- grid_regular(mtry(range = c(1,8)), trees(range = c(200,400)), min_n(range = c(2,8)), levels = 8)
```

## Question 7
```{r}
#tune_forest <- tune_grid(
#  pokemon_forest_wkflow,
#  resamples = pokemon.fold,
#  grid = param1_grid,
#  metrics = metric_set(roc_auc)
#)
#write_rds(tune_forest, file = 'tune_forest.rds')
tune_forest <- read_rds(file = 'tune_forest.rds')
autoplot(tune_forest)
```
higher mtry will lead to higher performance. 

## Question 8
```{r}
collect_metrics(tune_forest)
best_model2 <- select_best(tune_forest, metric = 'roc_auc')
pokemon_forest_best <- finalize_workflow(pokemon_forest_wkflow, best_model2)
pokemon_forest_best_fit <- fit(pokemon_forest_best, data = pokemon.training)
random_forest <- augment(pokemon_forest_best_fit, pokemon.training, type = 'prob') %>% 
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
random_forest
```

## Question 9
```{r}
extract_fit_engine(pokemon_forest_best_fit) %>% 
vip()
```
sp_atk is the most useful variable. 

## Question 10
```{r}
pokemon_boost_model <- boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")
pokemon_boost_wkflow <- workflow() %>% 
  add_model(pokemon_boost_model) %>% 
  add_recipe(pokemon.recipe)
param2_grid <- grid_regular(trees(range(c(10,2000))), levels = 10)
#tune_boost <- tune_grid(
#  pokemon_boost_wkflow,
#  resamples = pokemon.fold,
#  grid = param2_grid,
#  metrics = metric_set(roc_auc)
#)
#write_rds(tune_boost, file = 'tune_boost.rds')
tune_boost <- read_rds('tune_boost.rds')
autoplot(tune_boost)
```
```{r}
collect_metrics(tune_boost)
best_model3 <- select_best(tune_boost, metric = 'roc_auc')
pokemon_boost_best <- finalize_workflow(pokemon_boost_wkflow, best_model3)
pokemon_boost_best_fit <- fit(pokemon_boost_best, data = pokemon.training)
boosted_tree <- augment(pokemon_boost_best_fit, pokemon.training, type = 'prob') %>% 
  roc_auc(truth=type_1, estimate = .pred_Bug:.pred_Water)
boosted_tree
```

## Question 11
```{r}
bind_rows(pruned_tree, random_forest, boosted_tree) %>% 
  tibble() %>% 
  mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one. 
#pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>% 
confmax <- augment(pokemon_forest_best_fit, pokemon.testing, type = 'prob') 
roc_auc(confmax, truth = type_1, estimate = .pred_Bug:.pred_Water)
conf_mat(confmax, truth = type_1, estimate = .pred_class) %>% 
  autoplot('heatmap')
roc_curve(confmax, truth = type_1, estimate = .pred_Bug:.pred_Water) %>%
  autoplot()
```
random forest model performs best and pruned tree model performs worest. 
## Pstat231 Only
## Question 12
```{r}
set.seed(3456)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone_fold <- vfold_cv(abalone_training, v = 5, strata = age)  #cross validation
```
```{r}
abalone_recipe <- recipe(age~., data = abalone_training) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ shucked_weight:starts_with('type')+
                  diameter:longest_shell+
                  shell_weight:shucked_weight) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

```{r}
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine('ranger') %>% 
  set_mode('regression')
abalone_wkflow <- workflow() %>% 
  add_model(abalone_rand_forest) %>% 
  add_recipe(abalone_recipe)
forest_tune <- grid_regular(mtry(range = c(1,8)), trees(range = c(200,400)), min_n(range = c(2,8)), levels = 10)
```
```{r}
#abalone_tune_forest <- tune_grid(
#  abalone_wkflow,
#  resamples = abalone_fold,
#  grid = forest_tune,
#  metrics = metric_set(rmse)
#)
#write_rds(abalone_tune_forest, file = 'abalone_tune_forest.rds')
abalone_tune_forest <- read_rds('abalone_tune_forest.rds')
autoplot(abalone_tune_forest)
```
```{r}
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
abalone_finalwkflow <- finalize_workflow(abalone_wkflow, abalone_best_model)
abalone_best_fit <- fit(abalone_finalwkflow, data = abalone_training) %>% 
augment(abalone_testing, type = 'class') %>% 
  rmse(truth = age, estimate = .pred)
abalone_best_fit
```





























