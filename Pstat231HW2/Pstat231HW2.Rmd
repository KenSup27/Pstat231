---
title: "Pstat231HW2"
author: "Wentao Yu"
date: '2022-10-04'
output:
  html_document: default
  pdf_document: default
---
```{r load package, include=FALSE}
library(yardstick)
library(tidymodels)
library(tidyverse)
```

##### Question 1
```{r read dataset Question 1}
data1 <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/Pstat131/homework-2/homework-2/data/abalone.csv') # import dataset
#data1 <- read.csv('D:/Github/Pstat131/homework-2/homework-2/data/abalone.csv')
age <- data1[9]+1.5 # extract the rings column since age = rings+1.5
data2 <- cbind(data1,age) # add a new column into the dataframe
names(data2)[10] <- 'age' # rename the new column as age
ggplot(data2, aes(age))+
  geom_histogram(col='white', binwidth = 1) # plot the age column using histogram to make it access.
```
The distribution of age is more likely a normal distribution with positive skew.

##### Question 2
```{r Question 2 Data Split}
set.seed(4177) # set seed to make sure the output is stable
data3 = subset(data2, select = -c(rings)) # new dataframe exclude rings
abalone_split <- initial_split(data3, prop = 0.80) # split the data set, then what is the appropriate percentage, may need a new data set since we cannot include rings and age????
abalone_training <- training(abalone_split) # this is training data set
abalone_testing <- testing(abalone_split) # this is testing data set
```

##### Question 3
```{r recipe abalone}
#simple_abalone_recipe <- recipe(age~.,data = abalone_training)
abalone_recipe <- recipe(age~., data = abalone_training) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ shucked_weight:starts_with('type')+
                  diameter:longest_shell+
                  shell_weight:shucked_weight) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
summary(abalone_recipe)
```
We cannot include ring as the predictor of age since they have strong linear relationship as age = ring+1.5

##### Question 4
```{r lm engine}
lm_model <- linear_reg() %>%
  set_engine('lm') # create and store a linear regression object
lm_model
```

##### Question 5
```{r}
lm_workflow <- workflow() %>% # set up a new workflow
  add_model(lm_model) %>% # add the linear model from question 4
  add_recipe(abalone_recipe) # add the recipe from question 3
lm_workflow
```

##### Question 6
```{r fit linear model}
#create a new dataframe including the question conditions
type <- c('F')
longest_shell <- 0.50
diameter <- 0.10
height <- 0.30
whole_weight <- 4
shucked_weight <- 1
viscera_weight <- 2
shell_weight <- 1
hypo1 <- data.frame(type, longest_shell, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight)
lm_fit <- fit(lm_workflow, abalone_training) # fit the training data
predict(lm_fit, hypo1) # predict the age using fitted training data from a new dataframe
```
##### Question 7
```{r}
abalone_training_res <- predict(lm_fit, new_data = abalone_training %>% select(-age))
abalone_training_res <- bind_cols(abalone_training_res, abalone_training %>% select(age))
abalone_matrics <- metric_set(rsq, rmse, mae)
abalone_matrics(abalone_training_res,truth = age, estimate = .pred)
```


##### Required For 231 Students
##### Question 8
Var($\hat{f}(x_0)$) is the reproducible errors.

Var($\epsilon$) is the irreducible error. 

##### Question 9
According to day_1_131_231, pg.70, we could take $\hat{f}(x_0)=E[Y|X=x_0]$, then $E[(\hat{f}(x_0)-E\hat{f}(x_0))^2]=[E[\hat{f}(x_0)]-\hat{f}(x_0)]^2=0$.\
In this case, $Var(\hat{f}(x_0))=[Bias(\hat{f}(x_0))]^2=0$\
But, we cannot control $Var(\epsilon)$ since this is the irreducible error. Thus, the expected test error is always at least as large as the irreducible error.

##### Question 10
$Bias(\hat{f}(x_0))=E[\hat{f}(x_0)]-f(x_0) -> [Bias(\hat{f}(x_0))]^2=[E[\hat{f}(x_0)]-f(x_0)]^2$\
According to day_1_131_231, pg.70, $Y=f(X)+\epsilon$ -> f(X) is non-random and $\epsilon$ is a zero-mean noise. Thus, E[f(X)]=f(X) and E($\epsilon$)=0. In this case, $Var(\epsilon)=E(\epsilon^2)$ and $y_0$ in the bias-variance tradeoff function is $f(x_0)+\epsilon$. \
\begin{equation*}
\begin{split}
E[(y_0-\hat{f}(x_0)^2] &= E[(f(x_0)+\epsilon+\hat{f}(x_0))^2]\\
                       &= E[f(x_0)-\hat{f}(x_0)^2]+E[\epsilon^2]+2E[(f(x_0)-\hat{f}(x_0))]E[\epsilon]\\
                       &= E[f(x_0)-\hat{f}(x_0)^2]+Var(\epsilon)
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
E[f(x_0)-\hat{f}(x_0)^2] &= E[((f(x_0)-E[\hat{f}(x_0)])-(\hat{f}(x_0)-E[\hat{f}(x_0)]))^2]\\
                         &= E[(E[\hat{f}(x_0)]-f(x_0))^2]+E[(\hat{f}(x_0)-E[\hat{f}(x_0)])^2]\\
                         &  -2E[(f(x_0)-E[\hat{f}(x_0)])(\hat{f}(x_0)-E[\hat{f}(x_0)])]\\
                         &= (E[\hat{f}(x_0)]-f(x_0))^2+E[(\hat{f}(x_0)-E[\hat{f}(x_0)])^2]\\
                         &  -2(f(x_0)-E[\hat{f}(x_0)])E[(\hat{f}(x_0)-E[\hat{f}(x_0)])]\\
                         &= (E[\hat{f}(x_0)]-f(x_0))^2+E[(\hat{f}(x_0)-E[\hat{f}(x_0)])^2]\\
                         &  -2(f(x_0)-E[\hat{f}(x_0)])(E[\hat{f}(x_0)]-E[\hat{f}(x_0)])\\
                         &= (E[\hat{f}(x_0)]-f(x_0))^2+E[(\hat{f}(x_0)-E[\hat{f}(x_0)])^2]\\
                         &= [bias(\hat{f}(x_0))]^2 + Var(\hat{f}(x_0))
\end{split}
\end{equation*}
In this case,
\begin{equation*}
\begin{split}
E[(y_0-\hat{f}(x_0))^2] &= E[(f(x_0)+\epsilon+\hat{f}(x_0))^2]\\
                       &= E[f(x_0)-\hat{f}(x_0)^2]+E[\epsilon^2]+2E[(f(x_0)-\hat{f}(x_0))]E[\epsilon]\\
                       &= E[f(x_0)-\hat{f}(x_0)^2]+Var(\epsilon)\\
                       &= [bias(\hat{f}(x_0))]^2 + Var(\hat{f}(x_0))+Var(\epsilon)
\end{split}
\end{equation*}









