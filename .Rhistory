pokemon_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger', importance = "impurity") %>%
set_mode('classification')
pokemon_forest_wkflow <- workflow() %>%
add_model(pokemon_forest_model) %>%
add_recipe(pokemon.recipe)
param1_grid <- grid_regular(mtry(range = c(1,8)), trees(range = c(1,8)), min_n(range = c(2,8)), levels = 8) # how to choose the tune range???
result <- bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
#select_best(result, metric = '.estimate') # how to use select best???
result
result <- bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
select_best(result, metric = 'roc_auc') # how to use select best???
result
augment(pokemon_boost_best_fit, pokemon.training, type = 'prob')
augment(pokemon_boost_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth=type_1, estimate = .pred_Bug:.pred_Water)
collect_metrics(tune_boost)
best_model3
best_model3 <- select_best(tune_boost, metric = 'roc_auc')
collect_metrics(tune_boost)
best_model3
collect_metrics(tune_boost)
best_model3
collect_metrics(tune_boost)
collect_metrics(tune_boost[4,])
collect_metrics(tune_boost)[4,]
best_model3 <- select_best(tune_boost, metric = 'roc_auc')
pokemon_boost_best <- finalize_workflow(pokemon_boost_wkflow, best_model3)
pokemon_boost_best_fit <- fit(pokemon_boost_best, data = pokemon.training)
boosted_tree <- augment(pokemon_boost_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth=type_1, estimate = .pred_Bug:.pred_Water)
boosted_tree
collect_metrics(tune_boost)[4,]
collect_metrics(tune_boost)
library(xgboost)
library(vip)
library(ranger)
library(rpart.plot)
library(corrplot)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
library(janitor)
library(glmnet)
library(ISLR)
library(ISLR2)
tidymodels_prefer()
## table reading
df0 <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-5/data/Pokemon.csv')
df <- clean_names(df0)
df1 <- dplyr::filter(df, type_1 %in% c('Bug','Fire','Grass','Normal','Water','Psychic')) %>%
mutate(type_1 = factor(type_1),
legendary = factor(legendary),
generation = factor(generation)) ## select the required outcomes and factor type_1, legendary, and generation.
set.seed(721) ## set seed to stabilize the outcome
pokemon.split <- initial_split(df1, prop = 0.8, strata = type_1)
pokemon.training <- training(pokemon.split)
pokemon.testing <- testing(pokemon.split)
pokemon.fold <- vfold_cv(pokemon.training, v=5, strata = type_1) ## return the distribution of strata variable to each fold.
length(pokemon.training)
dim(pokemon.training)
pokemon.recipe <- recipe(type_1 ~ legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data = pokemon.training) %>%
step_dummy(c(legendary,generation)) %>%
step_center() %>%  ## do we need all_predictors()???
step_scale()
pokemon.training
cor.pokemon <- pokemon.training %>%
select(-c(total, name, type_1, type_2, generation, legendary)) %>%  # eliminate chr variables
correlate() # create correlation data frame
rplot(cor.pokemon) # create correlation plot
tree.pokemon <- decision_tree() %>%
set_engine('rpart') %>%
set_mode('classification')
pokemon_wkflow <- workflow() %>%
add_model(tree.pokemon %>% set_args(cost_complexity = tune())) %>%
add_recipe(pokemon.recipe)
param_grid <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)
tune_tree <- tune_grid(
pokemon_wkflow,
resamples = pokemon.fold,
grid = param_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_tree)
collect_metrics(tune_tree)
best_model <- select_best(tune_tree, metric = 'roc_auc')
pokemon_tree_best <- finalize_workflow(pokemon_wkflow, best_model)
pokemon_tree_best_fit <- fit(pokemon_tree_best, data = pokemon.training)
pruned_tree <- augment(pokemon_tree_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
pruned_tree
pokemon_tree_best_fit %>%
extract_fit_engine() %>%
rpart.plot()
# warning ???
pokemon_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger', importance = "impurity") %>%
set_mode('classification')
pokemon_forest_wkflow <- workflow() %>%
add_model(pokemon_forest_model) %>%
add_recipe(pokemon.recipe)
param1_grid <- grid_regular(mtry(range = c(1,8)), trees(range = c(1,8)), min_n(range = c(2,8)), levels = 8)
tune_forest <- tune_grid(
pokemon_forest_wkflow,
resamples = pokemon.fold,
grid = param1_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_forest)
collect_metrics(tune_forest)
best_model2 <- select_best(tune_forest, metric = 'roc_auc')
pokemon_forest_best <- finalize_workflow(pokemon_forest_wkflow, best_model2)
pokemon_forest_best_fit <- fit(pokemon_forest_best, data = pokemon.training)
random_forest <- augment(pokemon_forest_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
random_forest
extract_fit_engine(pokemon_forest_best_fit) %>%
vip()
pokemon_boost_model <- boost_tree(trees = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
pokemon_boost_wkflow <- workflow() %>%
add_model(pokemon_boost_model) %>%
add_recipe(pokemon.recipe)
param2_grid <- grid_regular(trees(range(c(10,2000))), levels = 10)
tune_boost <- tune_grid(
pokemon_boost_wkflow,
resamples = pokemon.fold,
grid = param2_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_boost)
collect_metrics(tune_boost)
best_model3 <- select_best(tune_boost, metric = 'roc_auc')
pokemon_boost_best <- finalize_workflow(pokemon_boost_wkflow, best_model3)
pokemon_boost_best_fit <- fit(pokemon_boost_best, data = pokemon.training)
boosted_tree <- augment(pokemon_boost_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth=type_1, estimate = .pred_Bug:.pred_Water)
boosted_tree
result <- bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
select_best(result, metric = 'roc_auc') # how to use select best???
result <- bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
#select_best(result, metric = 'roc_auc') # how to use select best???
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
abalone
library(xgboost)
library(vip)
library(ranger)
library(rpart.plot)
library(corrplot)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
library(janitor)
library(glmnet)
library(ISLR)
library(ISLR2)
tidymodels_prefer()
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
abalone_split <- initial_split(abalone_split, prop = 0.8, strata = survived)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
abalone_split <- initial_split(abalone, prop = 0.8, strata = survived)
abalone
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
abalone_split <- initial_split(abalone, prop = 0.8, strata = age)
abalone
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(data1,age) # add a new column into the dataframe
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(data1,age) # add a new column into the dataframe
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(data2)[10] <- 'age' # rename the new column as age
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone_fold <- vfold_cv(abalone_training, v = 5)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone_fold <- vfold_cv(abalone_training, v = 5, strata = age)
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
abalone_recipe <- recipe(age~., data = abalone_training) %>%
step_dummy(all_nominal_predictors()) %>%
step_interact(terms = ~ shucked_weight:starts_with('type')+
diameter:longest_shell+
shell_weight:shucked_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
abalone_wkflow <- workflow() %>%
add_model(abalone_rand_forest) %>%
add_recipe(abalone_recipe)
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
abalone_wkflow <- workflow() %>%
add_model(abalone_rand_forest) %>%
add_recipe(abalone_recipe)
forest_tune <- grid_regular(mtry(range = c(1,8)), trees(range = c(1,8)), min_n(range = c(2,8)), levels = 10)
abalone_tune_forest <- tune_grid(
abalone_wkflow,
resamples = abalone_fold,
grid = forest_tune,
metrics = metric_set(rmse)
)
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger') %>%
set_mode('regression')
abalone_wkflow <- workflow() %>%
add_model(abalone_rand_forest) %>%
add_recipe(abalone_recipe)
forest_tune <- grid_regular(mtry(range = c(1,8)), trees(range = c(1,8)), min_n(range = c(2,8)), levels = 10)
abalone_tune_forest <- tune_grid(
abalone_wkflow,
resamples = abalone_fold,
grid = forest_tune,
metrics = metric_set(rmse)
)
autoplot(abalone_tune_forest)
collect_metrics(abalone_tune_forest)
collect_metrics(abalone_tune_forest)
select_best(abalone_tune_forest, metric = 'rmse')
result
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one.
pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>%
augment(pokemon.testing, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one.
pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>%
augment(pokemon.testing, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
pokemon_boost_tree_testing
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one.
#pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>%
augment(pokemon_boost_best_fit, pokemon.testing, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
abalone_finalwkflow <- finalize_workflow(abalone_wkflow, abalone_best_model)
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
abalone_finalwkflow <- finalize_workflow(abalone_wkflow, abalone_best_model)
abalone_best_fit <- fit(abalone_finalwkflow, data = abalone_training) %>%
augment(abalone_testing, type = 'class') %>%
rmse(truth = age, estimate = .pred)
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
abalone_finalwkflow <- finalize_workflow(abalone_wkflow, abalone_best_model)
abalone_best_fit <- fit(abalone_finalwkflow, data = abalone_training) %>%
augment(abalone_testing, type = 'class') %>%
rmse(truth = age, estimate = .pred)
abalone_best_fit
library(xgboost)
library(vip)
library(ranger)
library(rpart.plot)
library(corrplot)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
library(janitor)
library(glmnet)
library(ISLR)
library(ISLR2)
tidymodels_prefer()
## table reading
df0 <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-5/data/Pokemon.csv')
df <- clean_names(df0)
df1 <- dplyr::filter(df, type_1 %in% c('Bug','Fire','Grass','Normal','Water','Psychic')) %>%
mutate(type_1 = factor(type_1),
legendary = factor(legendary),
generation = factor(generation)) ## select the required outcomes and factor type_1, legendary, and generation.
set.seed(721) ## set seed to stabilize the outcome
pokemon.split <- initial_split(df1, prop = 0.8, strata = type_1)
pokemon.training <- training(pokemon.split)
pokemon.testing <- testing(pokemon.split)
pokemon.fold <- vfold_cv(pokemon.training, v=5, strata = type_1) ## return the distribution of strata variable to each fold.
length(pokemon.training)
dim(pokemon.training)
pokemon.recipe <- recipe(type_1 ~ legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data = pokemon.training) %>%
step_dummy(c(legendary,generation)) %>%
step_center() %>%  ## do we need all_predictors()???
step_scale()
pokemon.training
cor.pokemon <- pokemon.training %>%
select(-c(total, name, type_1, type_2, generation, legendary)) %>%  # eliminate chr variables
correlate() # create correlation data frame
rplot(cor.pokemon) # create correlation plot
tree.pokemon <- decision_tree() %>%
set_engine('rpart') %>%
set_mode('classification')
pokemon_wkflow <- workflow() %>%
add_model(tree.pokemon %>% set_args(cost_complexity = tune())) %>%
add_recipe(pokemon.recipe)
param_grid <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)
tune_tree <- tune_grid(
pokemon_wkflow,
resamples = pokemon.fold,
grid = param_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_tree)
collect_metrics(tune_tree)
best_model <- select_best(tune_tree, metric = 'roc_auc')
pokemon_tree_best <- finalize_workflow(pokemon_wkflow, best_model)
pokemon_tree_best_fit <- fit(pokemon_tree_best, data = pokemon.training)
pruned_tree <- augment(pokemon_tree_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
pruned_tree
pokemon_tree_best_fit %>%
extract_fit_engine() %>%
rpart.plot()
# warning ???
pokemon_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger', importance = "impurity") %>%
set_mode('classification')
pokemon_forest_wkflow <- workflow() %>%
add_model(pokemon_forest_model) %>%
add_recipe(pokemon.recipe)
param1_grid <- grid_regular(mtry(range = c(1,8)), trees(range = c(200,400)), min_n(range = c(2,8)), levels = 8)
tune_forest <- tune_grid(
pokemon_forest_wkflow,
resamples = pokemon.fold,
grid = param1_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_forest)
collect_metrics(tune_forest)
best_model2 <- select_best(tune_forest, metric = 'roc_auc')
pokemon_forest_best <- finalize_workflow(pokemon_forest_wkflow, best_model2)
pokemon_forest_best_fit <- fit(pokemon_forest_best, data = pokemon.training)
random_forest <- augment(pokemon_forest_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
random_forest
extract_fit_engine(pokemon_forest_best_fit) %>%
vip()
pokemon_boost_model <- boost_tree(trees = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
pokemon_boost_wkflow <- workflow() %>%
add_model(pokemon_boost_model) %>%
add_recipe(pokemon.recipe)
param2_grid <- grid_regular(trees(range(c(10,2000))), levels = 10)
tune_boost <- tune_grid(
pokemon_boost_wkflow,
resamples = pokemon.fold,
grid = param2_grid,
metrics = metric_set(roc_auc)
)
autoplot(tune_boost)
collect_metrics(tune_boost)
best_model3 <- select_best(tune_boost, metric = 'roc_auc')
pokemon_boost_best <- finalize_workflow(pokemon_boost_wkflow, best_model3)
pokemon_boost_best_fit <- fit(pokemon_boost_best, data = pokemon.training)
boosted_tree <- augment(pokemon_boost_best_fit, pokemon.training, type = 'prob') %>%
roc_auc(truth=type_1, estimate = .pred_Bug:.pred_Water)
boosted_tree
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one.
#pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>%
augment(pokemon_boost_best_fit, pokemon.testing, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
abalone <- read.csv('/Users/wentaoyu/Documents/UCSB File/Stats/Pstat131/HWs/homework-6/data/abalone.csv')
age <- abalone[9]+1.5 # extract the rings column since age = rings+1.5
abalone2 <- cbind(abalone,age) # add a new column into the dataframe
names(abalone2)[10] <- 'age' # rename the new column as age
abalone2_split <- initial_split(abalone2, prop = 0.8, strata = age)
abalone_training <- training(abalone2_split)
abalone_testing <- testing(abalone2_split)
abalone_fold <- vfold_cv(abalone_training, v = 5, strata = age)  #cross validation
abalone_recipe <- recipe(age~., data = abalone_training) %>%
step_dummy(all_nominal_predictors()) %>%
step_interact(terms = ~ shucked_weight:starts_with('type')+
diameter:longest_shell+
shell_weight:shucked_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
abalone_rand_forest <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine('ranger') %>%
set_mode('regression')
abalone_wkflow <- workflow() %>%
add_model(abalone_rand_forest) %>%
add_recipe(abalone_recipe)
forest_tune <- grid_regular(mtry(range = c(1,8)), trees(range = c(200,400)), min_n(range = c(2,8)), levels = 10)
abalone_tune_forest <- tune_grid(
abalone_wkflow,
resamples = abalone_fold,
grid = forest_tune,
metrics = metric_set(rmse)
)
autoplot(abalone_tune_forest)
collect_metrics(abalone_tune_forest)
abalone_best_model <- select_best(abalone_tune_forest, metric = 'rmse')
abalone_finalwkflow <- finalize_workflow(abalone_wkflow, abalone_best_model)
abalone_best_fit <- fit(abalone_finalwkflow, data = abalone_training) %>%
augment(abalone_testing, type = 'class') %>%
rmse(truth = age, estimate = .pred)
abalone_best_fit
abalone_training
ï¼Ÿmetric_set()
?metric_set
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
library(xgboost)
library(vip)
library(ranger)
library(rpart.plot)
library(corrplot)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
library(janitor)
library(glmnet)
library(ISLR)
library(ISLR2)
tidymodels_prefer()
bind_rows(pruned_tree, random_forest, boosted_tree) %>%
tibble() %>%
mutate(model = c('pruned tree model', 'random forest model', 'boost tree model'), .before = .metric)
# Boost Tree model is the best one.
#pokemon_boost_tree_testing <- fit(pokemon_boost_best, data = pokemon.testing) %>%
augment(pokemon_forest_best_fit, pokemon.testing, type = 'prob') %>%
roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
